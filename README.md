* content
{:toc}




> 几家出版社
>
> **ACM**  美国计算机协会（Association for Computing Machinery; [ACM](https://baike.baidu.com/item/ACM/64774)），成立于1947年，是一个国际性的科技教育组织，是世界上第一个科学性及教育性计算机学会，总部设在美国纽约。有很多特别兴趣小组（Special Interest Groups; SIGs）：包括信息技术37个细分领域的特别兴趣小组供会员选择加入
>
> **IEEE** 电气与电子工程师协会（Institute of Electrical and Electronics Engineers），简称IEEE，总部位于美国纽约，是一个国际性的电子技术与信息科学工程师的协会，也是全球最大的非营利性专业技术学会
>
> **USENIX**  USENIX协会是一个非营利组织，致力于支持先进的计算系统社区并扩大创新研究的范围。我们以组织会议和出版研究而闻名，但我们最大的优势在于建立计算系统社区。我们以多种方式代表我们社区的利益，包括我们与计算研究协会的专业联系。自 1975 年以来，USENIX 将致力于计算世界前沿的工程师、系统管理员、SRE、研究人员和技术人员聚集在一起。USENIX 会议已成为展示和讨论有关计算系统各个方面发展的最先进信息的重要会议场所。
>
> **Springer** springer是[Springer-Verlag](https://baike.baidu.com/item/Springer-Verlag)的简称，[德国](https://baike.baidu.com/item/德国/147953)Springer-Verlag(斯普林格)出版社是世界上最大的科技出版社之一，它有着170多年发展历史，以出版学术性出版物而闻名于世，它也是最早将纸本期刊做成电子版发行的出版商。德国斯普林格(Springer-Verlag)通过SpringerLink系统提供其学术期刊及电子图书的在线服务，该数据库包括了各类期刊、丛书、图书、参考工具书以及回溯文档。

# CCFA

## 会议

### ASPLOS

> | 摘要截止日期        | 全文截止日期        |
> | ------------------- | ------------------- |
> | 2022 年 10 月 13 日 | 2022 年 10 月 20 日 |

#### 2022

> 2022.2.28-3.14
>
> 有专门的关于serverless的会议主题

##### 1.IceBreaker: Warming Serverless Functions Better with Heterogeneity  [9]

**摘要：**

无服务器计算是一种新兴的计算模式，它依赖于在预期执行前的 "预热 "功能，以便为用户提供更快、更经济的服务。不幸的是，**预热功能可能是不准确的，并且在预热期间会产生令人望而却步的昂贵成本（即函数保活成本）**。在本文中，**我们介绍了IceBreaker，这是一种新颖的技术，通过用异构节点（昂贵和便宜）组成系统来减少服务时间和 "保活 "成本。IceBreaker的做法是，根据函数的时间变化概率，动态地确定在具有成本效益类型的节点来预热函数。通过采用异构性，IceBreaker 允许在相同的成本预算下拥有更多数量的节点，因此可以保活更多数量的函数，并减少高负载时的等待时间。**我们的实际系统评估证实，IceBreaker使用具有代表性的无服务器应用程序和行业级工作负载跟踪，将整体保持不变的成本降低了45％，执行时间降低了27％。IceBreaker是第一个采用并利用昂贵和便宜节点混合的想法来改善无服务器功能的服务时间和保持成本的技术--为研究人员和从业者开辟了一条在异构服务器上进行无服务器计算的新研究途径。

##### 2.INFless: a native serverless system for low-latency, high-throughput inference [3]

**摘要：**

现代网站越来越依赖机器学习（ML）来提高其业务效率。开发和维护ML服务会给开发者带来高昂的成本。虽然无服务器系统是一个很有前途的降低成本的解决方案，但我们**发现目前的通用无服务器系统不能满足ML服务的低延迟、高吞吐量的需求。**

虽然简单地 "修补 "通用无服务器系统并不能完全解决这个问题，但我们建议这样的系统应该将推理的特点与无服务器范式天然地结合起来。我们**提出了INFless，这是第一个针对ML领域的无服务器平台。它在CPU和加速器之间提供了一个统一的、异构的资源抽象，并利用内置的批处理和非统一的扩展机制实现了高吞吐量。**它还通过协调管理批处理排队时间、执行时间和**冷启动率**支持低延迟。我们使用本地集群测试平台和大规模模拟来评估INFless。实验结果表明，INFless在系统吞吐量上优于最先进的系统 2-5 倍，满足了ML服务的延时目标。

##### 3.FaaSFlow: enable efficient workflow execution for function-as-a-service [3]

**摘要：**

无服务器计算（Function-as-a-Service）通过在容器中运行函数（或 Lambda）提供细粒度的资源共享。依赖于数据的函数需要按照预先定义的逻辑进行调用，这就是所谓的**无服务器工作流**。然而，我们的调查显示，传统的基于 master-worker 工作流执行架构在无服务器背景下表现不佳。**master-worker工作流调度模式导致了一项重大开销，其中函数在主节点中触发并分配给工作节点执行。此外，worker 之间的数据移动也降低了吞吐量。**

为此，我们**提出了一种用于无服务器工作流执行的 worker-side 工作流计划模式**。根据该设计，我们实现了FaaSFlow，**以便在无服务器环境下实现高效的工作流执行**。**此外，我们提出了一个自适应存储库FaaStore，它能够在同一节点上的函数之间快速传输数据，而无需通过数据库**。实验结果表明，FaaSFlow有效地将工作流调度开销平均降低了74.6%，数据传输开销最多降低了95%。当网络带宽波动时，FaaSFlow-FaaStore可以减少23.0%的吞吐量下降，并且能够使网络带宽的利用率提高1.5-4倍。

##### 4.Serverless computing on heterogeneous computers [1]

**摘要：**

现有的无服务器计算平台是建立在**同构**计算机基础上的，限制了函数密度，并将无服务器计算限制在有限的场景中。我们介绍**Molecule，这是第一个利用异构计算机的无服务器计算系统**。Molecule使通用设备（如Nvidia DPU）和特定领域的加速器（如FPGA和GPU）都能用于无服务器应用，从而显著提高函数密度（高出50%）和应用性能（高达34.6倍）。为了实现这些结果，我们首先提出了XPU-Shim，这是一个分布式的Shim，用于弥补底层多操作系统（当使用通用设备时）和我们的无服务器运行时间（即Molecule）之间的差距。我们进一步介绍了矢量化沙箱，这是一种抽象硬件异构性的沙箱抽象（当使用特定领域的加速器时）。此外，我们还**回顾了关于启动和通信延迟的最先进的无服务器优化**，并克服了在异构计算机上实现这些优化的挑战。我们已经在实际平台上用Nvidia DPU和Xilinx FPGA实现了Molecule，并使用基准和实际应用对其进行了评估。

> 研讨会
>
> [vHive 教程及serverless讲解](https://www.youtube.com/playlist?list=PLVdxPJaekjWqBsEUwnrYRQCaMqvcDVsBE)

#### 2021 

> 2021.4.19-4.23

##### 1.Nightcore: efficient and scalable serverless computing for latency-sensitive, interactive microservices [3]

摘要：

微服务架构是一种流行的软件工程方法，用于构建灵活的大规模在线服务。无服务器计算或者函数计算提供了一个简单的无状态函数编程模型，它是实现微服务的无状态 RPC 处理程序的自然基础，是容器化 RPC 服务器的替代方案。但是，**当前的无服务器平台具有毫秒级的运行时开销，使其无法满足现有交互式微服务所需的严格的亚毫秒级延迟目标**。 

我们展示了 **Nightcore，这是一个具有微秒级开销的无服务器函数运行时，可在函数之间提供基于容器的隔离。 Nightcore 的设计仔细考虑了具有微秒级开销的各种因素，包括函数请求的调度、通信原语、I/O 的线程模型和并发函数执行。** Nightcore 目前支持用 C/C++、Go、Node.js 和 Python 编写的无服务器函数。我们的评估表明，在运行延迟敏感的交互式微服务时，Nightcore 实现了 1.36–2.93倍更高的吞吐量和高达 69% 的尾部延迟减少。

##### 2.FaasCache: keeping serverless computing alive with greedy-dual caching [8]

**摘要：**

函数即服务（也称为无服务器计算）有望彻底改变应用程序使用云资源的方式。但是，由于在开始执行之前初始化其代码和数据依赖关系的开销，函数会遇到冷启动问题。**在函数完成执行后保活函数可以减轻冷启动开销**。 Keep-alive 策略必须根据函数的资源和使用特性保持函数处于活动状态，**由于 FaaS 工作负载的多样性，这具有挑战性**。

我们的见解是，keep-alive 类似于缓存。与当前方法相比，**我们受缓存启发的 Greedy-Dual keep-alive 策略可以有效地将冷启动开销减少 3 倍以上。重用距离和命中率曲线等缓存概念也可用于自适应的服务器资源配置**，这可以将 FaaS 提供商对现实世界动态工作负载的资源需求降低 30%。我们在基于 OpenWhisk 的 FaasCache 系统中实施基于缓存的保活和资源配置策略。我们希望我们的缓存类比为未来的 FaaS 工作负载和平台打开了更多原则和优化的保活和资源配置技术的大门。

##### 3.Benchmarking, analysis, and optimization of serverless function snapshots [6]

**摘要：**

无服务器计算因其高可扩展性和灵活的即用即付计费模式而被迅速采用。在无服务器中，开发人员将他们的服务构建为一组函数，由各种事件（如点击）零星地调用。函数调用的时间间隔变化很大，致使提供者在每次调用时都要启动新的函数实例，从而导致严重的冷启动延迟，降低了用户体验。**为了减少冷启动延迟，业界已转向快照，即在磁盘上存储一个完全启动的函数的镜像**，与从头开始启动函数相比，调用速度更快。

这项工作引入了 **vHive，这是一个用于无服务器实验的开源框架**，旨在使研究人员能够在整个无服务器堆栈中进行研究和创新。使用 vHive，我们描述了一个最先进的基于快照的无服务器基础设施，它基于业界领先的Containerd协调框架和Firecracker管理程序技术。**我们发现从快照开始的函数的执行时间平均比同一个函数驻留在内存时高 95%。我们表明高延迟归因于频繁的缺页故障**，因为函数的状态一次一页地从磁盘进入客户内存。我们的分析进一步表明，**在同一函数的不同调用中，函数访问的是相同的稳定工作页面集**。通过利用这一洞察，我们**构建了 REAP，这是一种用于无服务器主机的轻量级软件机制，它可以记录函数的客户内存页的稳定工作集，并主动将其从磁盘预取到内存中。**与基线快照相比，REAP 将冷启动延迟平均减少了 3.7 倍。

#### 2020

> 2020.3.18-3.20

##### 1.Catalyzer: Sub-millisecond Startup for Serverless Computing with Initialization-less Booting [5]

**摘要：**

无服务器计算为高效软件开发提供了成本效益和弹性。要实现这一点，**Serverless 沙箱系统必须解决两个挑战：函数实例之间的强隔离，以及低启动延迟以保证用户体验。**虽然基于虚拟化的沙箱可以提供强隔离，但沙箱和应用程序的初始化会导致不可忽略的启动开销。传统沙箱系统由于其与应用程序无关的特性而在低延迟启动方面存在不足：它们只能通过管理程序和客户内核的定制来减少沙箱初始化的延迟，这是不充分的，不能缓解大部分的启动开销。

本文提出了 **Catalyzer，一种无服务器沙箱系统设计，提供强大的隔离和极快的功能启动**。 **Catalyzer 不是从头开始启动，而是从一个形式良好的检查点镜像中恢复一个基于虚拟化的函数实例，从而跳过关键路径上的初始化（无初始化）**。 Catalyzer 通过按需恢复用户级内存状态和系统状态来提高恢复性能。我们还**提出了一个新的 OS 原语 sfork（沙箱分叉），通过直接重用正在运行的沙箱实例的状态来进一步减少启动延迟**。从根本上说，Catalyzer 通过重用状态消除了初始化成本，从而实现了对各种无服务器函数的一般优化。评估表明，Catalyzer 将启动延迟降低了几个数量级，在最佳情况下实现了 < 1ms 的延迟，并显着降低了实际工作负载的端到端延迟。 **Catalyzer已被蚂蚁金服采用**，我们也分享了产业发展的经验教训。

> 该会议只有2020年之后才陆陆续续收录有关serverless的文章，之前的年限里是没有这方面的文章的

### USENIX ATC

> | 摘要截止日期       | 最终纸质文件截止日期 |
> | ------------------ | -------------------- |
> | 2022 年 1 月 13 日 | 2022 年 6 月 9 日    |

#### 2021

> 2021.7.14-7.16

##### 1.SONIC: Application-aware Data Passing for Chained Serverless Applications [3]

**摘要：**

数据分析应用程序越来越多地利用无服务器执行环境，因为它们易于使用，而且随用随付。这类应用的结构通常由多个函数组成，这些函数被串联起来形成一个工作流。**当前在函数之间交换中间（临时）数据的方法是通过远程存储（例如 S3），这会引入显着的性能开销**。我们比较了**三种数据传递方法**，我们称之为 VM-Storage、Direct-Passing 和 state-of-practice Remote-Storage。至关重要的是，我们表明，没有一种单一的数据传递方法在所有场景下都占主导地位，最优选择取决于动态因素，例如输入数据的大小、中间数据的大小、应用程序的并行度和网络带宽。我们提出了 **SONIC，一种优化应用程序性能和成本的数据传递管理器，通过为无服务器工作流 DAG 的每个边缘透明地选择最佳数据传递方法并实现通信感知函数放置。 SONIC 监控应用程序参数并使用简单的回归模型来相应地调整其混合数据传递。**我们将 SONIC 与 Open-Lambda 集成，并使用在无服务器环境中流行的三个分析应用程序评估 Amazon EC2 上的系统。与四个基准相比，SONIC 在各种条件下提供更低的延迟（原始性能）和更高的性能/美元：SAND、vanilla OpenLambda、OpenLambda with Pocket 和 AWS Lambda。

【以上三种方法在文件大小，并行度、网络带宽不同的情况下分别是最优方法，优先考虑VM，不可行或牺牲并行性的情况下考虑另外两种，如果并行度低，网络带宽小考虑直接传输，并行度高、网络带宽大考虑远程调用。】

##### 2.FaaSNet: Scalable and Fast Provisioning of Custom Serverless Container Runtimes at Alibaba Cloud Function Compute [7]

**摘要：**

无服务器计算或函数即服务 (FaaS) 通过允许用户部署细粒度函数，同时提供完全托管的资源配置和自动扩展，实现了一种构建和扩展应用程序的新方式。自定义 FaaS 容器支持越来越受欢迎，因为它可以更好地控制操作系统、版本控制和工具，以实现 FaaS 应用程序的现代化。然而，**提供快速的容器配置给 FaaS 提供商带来了不小的挑战，因为容器配置成本高昂，而且现实世界的 FaaS 工作负载表现出高度动态的模式**。

在本文中，我们设计了 FaaSNet，这是一个用于加速 FaaS 容器配置的高度可扩展的中间件系统。 **FaaSNet 由全球最大的云提供商之一阿里云函数计算的 FaaS 平台的工作负载和基础设施要求驱动**。**FaaSNet 通过轻量级的自适应函数树 (FT) 结构实现可扩展的容器配置。 FaaSNet 使用高效的 I/O 按需获取机制来进一步大规模降低配置成本**。我们在阿里云函数计算中实现并集成了 FaaSNet。评估结果表明，FaaSNet：（1）在 8.3 秒内完成在 1000 个虚拟机上配置 2500 个函数容器，（2）扩展速度比阿里云当前的 FaaS 平台和最先进的 P2P 容器注册中心(Kraken)快 13.4 倍和 16.3 倍 和(3) 维持突发工作负载的时间比优化基线少75.2%。

##### 3.Faastlane: Accelerating Function-as-a-Service Workflows [5]

**摘要：**

在 FaaS 工作流中，一组函数通过相互交互和交换数据来实现应用程序逻辑。现代 FaaS 平台在单独的容器中执行工作流的每个函数。**当工作流中的函数交互时，产生的延迟会减慢执行速度。**

**Faastlane通过努力将工作流的函数作为线程在容器实例的单一进程中执行，从而最大限度地减少了函数交互延迟，这通过简单的加载/存储指令简化了数据共享**。对于操作**敏感数据的FaaS工作流，Faastlane使用英特尔内存保护密钥（MPK）提供轻量级的线程级隔离域**。虽然线程便于共享，但Python和Node.js等语言的实现（广泛用于FaaS应用程序）不允许线程的并发执行。Faastlane动态地识别FaaS工作流中的并行机会，分叉进程（而不是线程）或产生新的容器实例，以并发执行工作流的并行功能。

我们在Apache OpenWhisk上实现了Faastlane，并表明它将工作流实例的速度提高了15倍，与OpenWhisk相比，将函数交互延迟降低了99.95%。

#### 2020

> 2020.7.15-7.17

##### 1.Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider [10]

**摘要：**

函数即服务（FaaS）作为一种在云中向无服务器后端部署计算的方式，已经越来越受欢迎。这种模式将分配和配置资源的复杂性转移给了云提供商，**云提供商必须以尽可能低的资源成本提供永远可用的资源的假象（即没有冷启动的快速函数调用）**。这样做需要供应商深入了解FaaS工作负载的特点。不幸的是，关于这些特征的公开信息几乎没有。因此，在本文中，我们**首先描述了Azure Functions的整个生产型FaaS工作负载的特点**。我们举例说明，大多数函数被调用的频率很低，但调用频率有8个数量级的范围。通过观察我们的特征，我们**提出了一个实用的资源管理策略，该策略可以大大减少函数冷启动的次数，同时花费的资源也比现行的策略少**。

##### 2.Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing [5]

**摘要：**

无服务器计算非常适用于大数据处理，因为它可以快速而廉价地扩展到成千上万的并行函数。**现有的无服务器平台将函数隔离在短暂的、无状态的容器中，防止它们直接共享内存。这迫使用户反复复制和序列化数据，增加了不必要的性能和资源成本**。我们认为需要一种新的轻量级隔离方法，它支持函数之间直接共享内存，并减少资源开销。

我们介绍了**Faaslets，这是一种用于高性能无服务器计算的新隔离抽象。Faaslets 使用 WebAssembly 提供的 \emph{software-fault isolation} (SFI) 隔离已执行函数的内存，同时允许内存区域在同一地址空间的函数之间共享。因此，当函数在同一台机器上共存时，Faaslets可以避免昂贵的数据移动。**我们的Faaslets的运行时间Faasm使用标准的Linux cgroups隔离其他资源，例如CPU和网络，并为网络、文件系统访问和动态加载提供低级别的POSIX主机接口。**为了减少初始化时间，Faasm从已经初始化的快照中恢复Faaslets**。我们将Faasm与一个标准的基于容器的平台进行了比较，结果表明，在训练机器学习模型时，它实现了 2 倍的加速，同时内存减少了 10 倍；为了服务于机器学习推理，Faasm 将吞吐量翻了一番，并将尾部延迟降低了 90%。

> 原文摘要就是 \emph{software-fault isolation} (SFI) 

#### 2018

> 2019年没有和serverless相关的内容
>
> 2018.7.11-7.13

##### 1.SOCK: Rapid Task Provisioning with Serverless-Optimized Containers [8]

无服务器计算承诺为应用程序提供成本节约和极致弹性。不幸的是，缓慢的应用和**容器初始化会损害无服务器平台上的常见延迟**。在这项工作中，我们**分析了Linux容器原语，确定了与存储和网络隔离有关的可扩展性瓶颈。我们还分析了GitHub上的Python应用程序，并表明导入许多流行的库会使启动时间增加约100ms**。基于这些发现，我们实现了**SOCK，一个为无服务器工作负载优化的容器系统**。小心避免了内核的可扩展性瓶颈，使SOCK的速度比Docker提高了18倍。一个广义的Zygote配置策略产生了额外的3倍速度。基于Zygotes的更复杂的三层缓存策略使SOCK的速度比没有Zygotes的SOCK提高了45倍。相对于AWS Lambda和OpenWhisk，在一个图像处理案例研究中，带有SOCK的OpenLambda分别减少了2.8倍和5.3倍的平台开销。

##### 2.Peeking Behind the Curtains of Serverless Platforms

无服务器计算是一种新兴的范式，其中应用程序的资源配置和扩展由第三方服务管理。例子包括AWS Lambda、Azure Functions和Google Cloud Functions。**在这些服务易于使用的API背后是不透明的、复杂的基础设施和管理生态系统**。我们从无服务器**客户的角度**出发，进行了迄今为止最大规模的测量研究，在这三种服务中启动了超过5万个功能实例，以描述其架构、性能和资源管理效率。我们解释了这些平台如何使用虚拟机或容器来隔离不同账户的功能，这具有重要的安全意义。我们**从可扩展性、冷启动延迟和资源效率等方面来描述性能**，重点包括AWS Lambda采用类似bin-packing的策略来最大化虚拟机的内存利用率，在AWS和Azure中可能出现函数之间的严重争用，以及谷歌有允许客户免费使用资源的bug。

##### 3.Understanding Ephemeral Storage for Serverless Analytics [4]

无服务器计算框架允许用户以高弹性和细粒度的资源计费启动数千个并发任务，而无需显式管理计算资源。虽然在物联网和网络微服务方面已经取得了成功，但人们对利用无服务器计算来运行**数据密集型工作**（如交互式分析）的兴趣越来越大。在**无服务器平台上运行分析工作负载的一个关键挑战是使不同执行阶段的任务能够通过共享数据存储在彼此之间有效地交流数据**。在本文中，我们**探讨了不同的云存储服务（如对象存储和分布式缓存）作为无服务器分析的远程存储的适用性**。我们的分析得出了指导无服务器云存储系统设计的关键见解，包括Flash存储的性能和成本效益，以满足无服务器应用的要求，以及需要一种按需付费的存储服务，以支持高度并行应用的高吞吐需求。

##### 4.SAND: Towards High-Performance Serverless Computing [8]

无服务器计算已经成为一种新的云计算范式，在这种范式中，一个应用由可以单独管理和执行的各个函数组成。然而，**现有的无服务器平台通常在独立的容器中隔离和执行函数，并不利用函数之间的相互作用来提高性能**。这些做法导致了函数执行的高启动延迟和低效的资源使用。本文介绍了SAND，一个新的无服务器计算系统，它**比现有的无服务器平台具有更低的延迟、更好的资源效率和更大的弹性**。为了实现这些特性，**SAND引入了两项关键技术。1）应用层面的沙箱 2）分层的消息总线。**我们已经实现并部署了一个完整的SAND系统。我们的结果表明，SAND的性能明显优于最先进的无服务器平台。例如，在一个常用的图像处理应用中，与Apache OpenWhisk相比，SAND实现了43%的速度提升。

## 期刊

### TC

#### 2022

> 2022.01

##### 1.λDNN: Achieving Predictable Distributed DNN Training With Serverless Architectures [3]

**摘要：**

无服务器计算正在成为云中分布式深度神经网络 (DDNN) 训练的一个有前途的范例，因为它允许用户将复杂的模型训练分解为多个函数，而无需管理虚拟机或服务器。尽管提供了更简单的资源接口（即功函数数量和内存大小），但**函数资源供应不足（供应不足或过度供应）很容易导致无服务器平台中不可预测的 DDNN 训练性能**。我们对 AWS Lambda 的实证研究表明，无服务器 DDNN 训练的这种不可预测的性能主要是由 Parameter Servers (PS) 的资源瓶颈和较小的本地批量大小造成的。在本文中，我们**设计并实现了 λλDNN，这是一种具有成本效益的函数资源供应框架，可为无服务器 DDNN 训练工作负载提供可预测的性能，同时节省供应功能的预算**。利用 PS 网络带宽和函数 CPU 利用率，我们构建了一个轻量级的分析型 DDNN 训练性能模型，以实现我们设计的 λλDNN 资源供应策略，从而保证 DDNN 训练性能与无服务器函数。 AWS Lambda 上的大量原型实验和互补的跟踪驱动模拟表明，与最先进的资源配置策略相比，λλDNN 可以提供可预测的 DDNN 训练性能，并节省高达 66.7% 的函数资源货币成本，但具有可接受的运行时开销。

### TPDS

#### 2021

> 2021.01

##### 1.Modeling and Optimization of Performance and Cost of Serverless Applications [6]

> ppt链接

**摘要：**

近年来，函数即服务 (FaaS) 和无服务器应用程序由于其高可扩展性、易于资源管理和按需付费定价模式而大幅增加。然而，云用户在将应用程序迁移到 serverless 模式时面临着实际问题，即**缺乏分析性能和计费模型，以及在有限的预算和所需的 serverless 应用程序服务质量之间进行权衡**。在本文中，我们通过提出和回答两个关于无服务器应用程序的性能和成本的预测和优化的研究问题来填补这一空白。我们**提出了一种新结构来正式定义无服务器应用程序工作流，然后实施分析模型来预测平均端到端响应时间和工作流成本**。因此，我们**提出了一种名为概率精炼关键路径贪心算法 (PRCP) 的启发式算法，该算法具有四种贪心策略来回答关于性能和成本的两个基本优化问题。我们通过对 AWS Lambda 和 Step Functions 进行实验来广泛评估所提出的模型**。我们的分析模型可以预测无服务器应用程序的性能和成本，准确率超过 98%。 PRCP 算法可以实现无服务器应用程序的最佳配置，平均准确率为 97%。

#### 2020

> 2020.01

##### 1.Automated Fine-Grained CPU Cap Control in Serverless Computing Platform [5]

**摘要：**

无服务器计算已经成为一种新的云计算执行模型，它将用户和应用程序开发人员从显式管理“物理”资源中解放出来，将这种资源管理负担留给服务提供商。在本文中，我们研究了多租户无服务器计算平台的资源分配问题，明确考虑了**包括突然激增在内的工作负载波动**。特别是，我们调查了租户（他们的应用程序）具有不同工作负载特征的这些平台中性能下降的不同根本原因。为此，我们**开发了一个细粒度的 CPU 上限控制解决方案作为资源管理器，可以动态调整具有相同/相似性能要求的应用程序（即应用程序组）的 CPU 使用限制（或 CPU 上限）**。 CPU 上限的调整主要适用于 serverless 计算平台的同地工作进程，以尽量减少资源争用，这是性能下降的主要来源。实际的调整决策是基于使用组感知调度算法的性能指标（例如，节流时间和队列长度）做出的。在我们的本地集群中进行的大量实验结果证实，即使在传入工作负载存在波动和突然激增的情况下，所提出的资源管理器也可以有效地消除显式预留计算能力的负担。我们通过将提议的资源管理器与实践中广泛使用的几种启发式方法进行比较来衡量其鲁棒性，包括增强版本的循环和最小长度队列调度策略，在实际场景驱动的各种工作负载强度下。值得注意的是，我们的资源管理器优于其他启发式方法，将偏度和平均响应时间分别降低了 44% 和 94%，同时它不会过度使用 CPU 资源。

##### 2.An Event-Driven Approach to Serverless Seismic Imaging in the Cloud [3]

**摘要：**

为高性能计算 (HPC) 调整云是一项具有挑战性的任务，因为 HPC 应用程序的软件依赖于快速的网络连接并且对硬件故障很敏感。因此，在许多情况下，使用云基础设施重建传统的 HPC 集群是一种将 HPC 应用程序迁移到云的不可行解决方案。作为通用提升和移位方法的替代方法，我们考虑了地震成像的特定应用，并展示了一种无服务器和事件驱动的方法，用于在云中运行该问题的大规模实例。我们的工作流程不是永久运行的计算实例，而是基于具有高吞吐量批处理计算和事件驱动计算的无服务器架构，其中计算资源只有在使用时才会运行。我们证明了这种方法非常灵活，并允许弹性和嵌套级别的并行化，包括用于求解基础偏微分方程的域分解。虽然事件驱动的方法会在计算资源反复重启时引入一些开销，但它本质上为实例关闭提供了弹性，并通过避免空闲实例来显着降低成本，从而使云成为本地集群的可行替代方案大尺度地震成像。

# CCFB

## 会议

### SoCC

>2022年截稿日期：2022.4.15

#### 2021

> 2021.11.1-11.4

##### 1.Faa$T: A Transparent Auto-Scaling Cache for Serverless Applications

**摘要：**

函数即服务 (FaaS) 已成为一种越来越流行的用户部署应用程序的方式，而无需管理底层基础设施。然而，现有的 FaaS 平台依赖远程存储来维护状态，限制了可以高效运行的应用程序集。 FaaS 平台最近的缓存工作试图解决这个问题，但未能成功：它忽略了 FaaS 应用程序的广泛不同的特性，不根据数据访问模式扩展缓存，或者需要对应用程序进行更改。为了解决这些限制，我们提出了 Faa**$** T，这是一种用于无服务器应用程序的透明自动扩展分布式缓存。每个应用程序都有自己的缓存。在函数执行并且应用程序变为非活动状态后，缓存会随应用程序从内存中卸载。在为下一次调用重新加载时，Faa**$**T 用可能被访问的对象预热缓存。除了传统的基于计算的扩展之外，Faa**$**T 还根据工作集和对象大小进行扩展，以管理缓存空间和 I/O 带宽。我们通过全面研究 Azure Functions 上的数据访问模式来激发我们的设计。我们为 Azure Functions 实施 Faa**$**T，并表明与最先进的缓存系统相比，Faa**$**T 可以将具有挑战性的应用程序的性能提高多达 92%（平均 57%），并为大多数用户降低成本，即必须支持额外的服务器资源的成本。

【FaaS程序缓存状态，自动拓展卸载】

##### 2.Atoll: A Scalable Low-Latency Serverless Platform

**摘要：**

随着面向用户的应用程序采用无服务器计算，无服务器平台良好的延迟性能已成为一项强大的基本要求。然而，由于其底层控制和数据平面的设计特别不适合具有不可预测的到达模式的短期功能，因此在今天的平台上很难实现这一点。我们展示了 Atoll，一个无服务器平台，它通过重新设计控制和数据平面来克服挑战。在 Atoll 中，每个应用程序都与延迟期限相关联。 Atoll 通过以下方式实现其每个应用程序请求的延迟目标：(a) 将集群划分为（半全局调度程序、工作池）对，(b) 执行截止时间感知调度和主动沙箱分配，以及 (c) 使用负载平衡层进行沙盒感知路由，并自动扩展每个应用程序的半全局调度程序。我们的结果表明，与最先进的替代方案相比，Atoll 将错过的最后期限减少了约 66 倍，尾部延迟减少了约 3 倍。

【低延迟无服务器平台】

##### 3.Kraken: Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms

**摘要：**

微服务的日益普及导致了基于在线云服务的应用激增，这些应用通常被建模为有向无环图（DAG），由几十到几百个微服务组成。这些应用绝大多数都是面向用户的，因此有严格的SLO要求。 无服务器函数具有较短的资源配置时间和即时可扩展性，是开发此类延迟关键型应用的合适人选。然而，现有的无服务器供应商并不了解应用DAG的工作流特性，在许多情况下导致容器的过度配置。在动态DAG的情况下，这种情况进一步加剧，因为应用的函数链并不事先知道。 在这些观察的激励下，我们提出了Kraken，一个工作流感知的资源管理框架，在确保符合SLO的前提下，最大限度地减少应用DAG的容器供应数量。我们在OpenFaaS上设计和实现Kraken，并在一个多节点Kubernetes管理的集群上对其进行评估。 我们使用DeathStarbench工作负载套件和真实世界的痕迹进行了广泛的实验评估，证明Kraken产生的容器数量减少了76%，从而提高了容器的利用率，与无服务器平台中使用的最先进的调度器相比，集群范围内的能量分别节省了4倍和48%。

【工作流感知资源管理，减少容器冗余】

##### 4.Mu: An Efficient, Fair and Responsive Serverless Framework for Resource-Constrained Edge Clouds

**摘要：**

无服务器计算平台简化了模块化软件函数的开发、部署和自动化管理。然而，现有的无服务器平台通常假定有一个超额的云，这使得它们不适合资源稀缺的边缘计算环境。在本文中，我们提出了一个重新设计的无服务器平台，全面解决了资源有限的边缘云中无服务器函数的关键挑战。

我们的Mu平台简洁地整合了无服务器平台的核心资源管理组件：自动缩放、负载均衡和放置。Mu中的每个工作节点都会在响应头中透明地传播服务速率和队列长度等指标，将这些信息反馈给负载均衡系统，使其能够更好地路由请求，并反馈给我们的自动缩放器，以预测工作负载的波动并主动满足SLO要求。 然后，来自自动调节器的数据被放置引擎用来考虑异质性和竞争功能之间的公平性，确保整体资源效率，并最大限度地减少资源碎片。我们将我们的设计作为Knative无服务器平台的一套扩展来实现，并展示了其在资源效率、公平性和响应时间方面的改进。

【资源受限的无服务器平台】

##### 5.Mind the Gap: Broken Promises of CPU Reservations in Containerized Multi-tenant Clouds

**摘要：**

容器化正变得越来越流行，但不幸的是，容器通常无法通过分配的资源提供预期的性能。在本文中，我们首先证明了在容器位于同一位置的多租户环境中，性能差异和退化是显着的（高达 5 倍）。然后，我们调查这种性能下降的根本原因。与通常认为这种降级是由资源争用和干扰引起的看法相反，我们发现容器预留的 CPU 数量与实际获得的 CPU 数量之间存在差距。根本原因在于当今Linux调度机制的设计选择，我们称之为Forced Runqueue Sharing和Phantom CPU Time。实际上，预留 CPU 资源的需求与 Completely Fair Scheduler 的工作节约性质之间存在根本冲突，这种矛盾阻碍了容器充分利用其请求的 CPU 资源。作为概念验证，我们在广泛使用的 Kubernetes 和 Linux 上实现了一种新的资源配置机制，以展示其潜在优势并为未来的调度程序重新设计提供启示。与现有调度程序相比，我们的概念验证将批处理和交互式容器化应用程序的性能提高了 5.6 倍和 13.7 倍。

##### 6.Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis

**摘要：**

在容器中运行的松散耦合和轻量级微服务正在逐渐取代单体应用程序。了解微服务的特性对于善用微服务架构至关重要。但是，目前还没有关于生产环境中的微服务及其相关系统的全面研究。在本文中，我们对阿里巴巴集群中的大规模微服务部署进行了深入的分析。我们的研究侧重于微服务依赖的表征及其运行时性能。我们对微服务调用图进行了深入剖析，以量化它们与数据并行作业的传统 DAG 之间的差异。特别是，我们观察到微服务调用图是重尾分布的，它们的拓扑类似于树，而且许多微服务都是热点。我们揭示了三种可用于优化微服务设计的有意义的调用依赖。我们对微服务运行时性能的调查表明，大多数微服务对 CPU 干扰比对内存干扰更敏感。为了合成更具代表性的微服务跟踪，我们建立了一个数学模型来模拟调用图。实验结果表明，我们的模型可以很好地保留从阿里巴巴跟踪中观察到的那些图属性。

##### 7.ServerMore: Opportunistic Execution of Serverless Functions in the Cloud

**摘要：**

无服务器计算允许客户将他们的工作提交到云中执行，资源供应由云提供商负责。无服务器函数通常是短暂的并且具有适度的资源需求，因此提供了通过与延迟敏感的客户工作负载共存来提高服务器利用率的机会。本文介绍了 ServerMore，这是一种服务器级资源管理器，可将客户无服务器作业与有服务器的客户 VM 有机地共存。 ServerMore 动态调节服务器上的 CPU、内存带宽和 LLC 资源，以确保 serverful 和 serverless 工作负载之间的托管不会影响应用程序尾部延迟。通过选择性地承认无服务器函数并推断黑盒服务器工作负载的性能，ServerMore 与之前的工作相比，资源利用率平均提高了 35.9% 至 245%；同时对服务器应用程序和无服务器功能的延迟影响最小。

【共存提高资源利用率】

##### 8.On Merits and Viability of Multi-Cloud Serverless

**摘要：**

无服务器计算是云行业中快速发展的范例，将函数设想为应用程序的计算构建块。云提供商不是强迫应用程序开发人员为其应用程序提供云资源，而是为每个函数“在后台”提供所需的资源。在这项工作中，我们设想虚拟无服务器提供商 (VSP) 来聚合无服务器产品。通过这样做，VSP 允许开发人员（和企业）摆脱供应商锁定问题，并通过每次适应性地利用最佳供应商来利用供应商之间的定价和性能差异，迫使供应商竞争以提供更便宜和更优质的服务。我们讨论了 VSP 的优点，并表明**与虚拟机相比，无服务器系统非常适合跨提供商聚合**。我们提出了一个 VSP 系统架构并实现了一个初始版本。通过实验评估，我们的初步结果表明，VSP 可以将最大持续吞吐量提高 1.2 倍至 4.2 倍，将 SLO 违规减少 98.8%，并将总调用成本提高 54%。

【无服务器函数的云际】

注：VSP 的提出有以下的理论基础：(1) 无服务器计算函数通常是轻量级的，因此可以“无痛”地在多家云计算提供商之间部署和迁移服务；(2) 存在较多开源的无服务器计算框架；(3) 已有的工作研究无服务器计算中函数性能的可预测性，有助于优化服务厂商的选择。

##### 9.Speedo: Fast dispatch and orchestration of serverless workflows

**摘要：**

将云应用结构化为相互作用的细粒度微服务的集合，使其具有可扩展性，并提供了对应用部分进行热升级的灵活性。目前无服务器计算（FaaS）的化身及其动态资源分配和自动扩展能力使其成为此类应用的首选部署模式。 随着微服务的粒度接近几毫秒的执行时间，再加上每秒接近数万个请求的负载，拥有低于一毫秒的低调度延迟对于跟上线路速率至关重要。当这些微服务是构成应用程序的工作流的一部分时，协调微服务执行顺序的协调器也需要以微秒级的延迟运行。 我们的观察显示，调度/编排延迟的最重要组成部分是请求从网络进入和离开用户空间所需的时间。由于今天的SmartNIC上存在大量的低功耗内核，要跟上这些高线速和严格的延迟预期，一种方法是在SmartNIC上靠近网络运行调度器和编排器。 FaaS调度器/协调器的短时短暂状态和低CPU突发要求的运行特性使它们成为从服务器卸载到NIC核心的理想候选者。这也带来了释放服务器CPU的其他好处。 我们在基于ASIC的Netronome Agilio智能网卡上实现了Speedo，我们的综合评估表明，Speedo在每秒10K个请求的负载下，将调度延迟从~150ms降至~140μs。

#### 2020

> 2020.10.19-20.21

##### 1.Wukong: a scalable and locality-enhanced framework for serverless parallel computing

**摘要：**

执行复杂的、突发并行的有向无环图 (DAG) 作业对无服务器执行框架提出了重大挑战，它需要以高吞吐量快速扩展和调度任务，同时最大限度地减少跨任务的数据移动。我们证明，对于无服务器并行计算，分散式调度使调度能够分布在可以并行调度任务的 Lambda 执行程序中，并带来多种好处，包括增强数据局部性、减少网络 I/O、自动资源弹性和提高成本效益.我们描述了我们新的无服务器并行框架 Wukong 在 AWS Lambda 上的实施和部署。我们表明 Wukong 实现了近乎理想的可扩展性，并行计算作业的执行速度高达 68.17 倍，网络 I/O 减少了多个数量级，与 numpywren 相比，租户端成本节省了 92.96%。

##### 2.Characterizing serverless platforms with serverlessbench

**摘要：**

无服务器计算保证了高生产力软件开发的自动可扩展性和成本效率（以“按需付费”的方式）。由于其优点，无服务器计算在云中推动了越来越多的新应用程序和服务。然而，这也带来了新的挑战，包括如何高效地设计高性能无服务器平台以及如何在平台上高效地编程。

本文提出了 ServerlessBench，这是一个用于表征无服务器平台的开源基准测试套件。它包括探索无服务器计算特征指标的测试用例，例如通信效率、启动延迟、无状态开销和性能隔离。我们已应用基准套件来评估最流行的无服务器计算平台，包括 AWS Lambda、Open-Whisk 和 Fn，并从研究中展示新的无服务器影响。例如，我们展示了将应用程序解耦为无服务器功能组合的场景，这有助于节省成本和提高性能，而无服务器计算中的“无状态”属性可能会损害无服务器功能的执行性能。这些影响形成了几个设计指南，可以帮助平台设计人员优化无服务器平台和应用程序开发人员设计最适合平台的功能。

##### 3.Photons: lambdas on a diet

**摘要：**

无服务器计算允许用户创建简短的无状态函数并同时调用数百个函数来处理大规模并行工作负载。我们观察到，即使无服务器函数的大部分足迹在其调用中是固定的——语言运行时、库和其他应用程序状态——今天的无服务器平台并没有利用这种冗余。这种低效率会产生一系列负面影响：更长的启动时间、更低的吞吐量、更高的延迟和更高的成本。为了缓解这些问题，我们构建了 Photons，这是一个利用工作负载并行性在同一运行时内共同定位同一函数的多个实例的框架。然后并发调用可以透明地共享运行时和应用程序状态，而不会影响执行安全。光子每次调用将函数的内存消耗减少 25% 到 98%，与当今的无服务器平台相比，性能没有下降。我们还表明，我们的方法可以将整体内存利用率降低 30%，将冷启动总数降低 52%。

##### 4.Serverless linear algebra

**摘要：**

数据中心分解为数据中心运营商和应用程序设计者提供了许多好处。然而，从以服务器为中心的模型切换到分解模型需要开发新的编程抽象，这些抽象可以实现高性能，同时受益于更大的弹性。为了探索数据中心分解的局限性，我们研究了一个几乎最大程度地受益于当前以服务器为中心的数据中心的应用领域：密集线性代数。我们构建了 NumPyWren，这是一个建立在分解的无服务器编程模型上的线性代数系统，以及 LAmbdaPACK，一种为高度并行的线性代数算法的无服务器执行而设计的配套领域特定语言。我们表明，对于矩阵乘法、奇异值分解、Cholesky 分解和 QR 分解等许多线性代数算法，NumPyWren 的性能（完成时间）在优化的以服务器为中心的 MPI 实现的 2 倍以内，并且具有计算效率（总 CPU 小时数）提高 15%，同时提供容错能力。

【应用】

##### 5.Kappa: a programming framework for serverless computing

**摘要：**

无服务器计算最近已成为在云上运行软件的新范例。在这种范式中，程序需要表示为一组短期任务，每个任务都可以在很短的有限时间内完成（例如，AWS Lambda 上的 15 分钟）。无服务器计算有利于云提供商——通过允许他们更好地利用资源——以及对用户——通过简化管理和实现更大的弹性。然而，开发在这种环境中运行的应用程序具有挑战性，需要用户适当地划分他们的代码，开发新的协调机制，并处理故障恢复。在本文中，我们提出了 Kappa，一个简化无服务器开发的框架。它使用检查点来处理 lambda 函数超时，并提供支持并行计算和协调的并发机制。

##### 6.Sequoia: enabling quality-of-service in serverless computing

**摘要：**

无服务器计算是一种快速发展的范式，可以轻松利用云的力量。借助无服务器计算，开发人员只需向云提供商提供事件驱动的功能，并且提供商可以无缝扩展函数调用以满足事件触发发生时的需求。由于当前和未来的无服务器产品支持各种无服务器应用程序，因此管理无服务器工作负载的有效技术成为一个重要问题。这项工作检查了云提供商当前的管理和调度实践，发现了许多问题，包括应用程序运行时间膨胀、功能下降、分配效率低下以及其他未记录和意外行为。为了解决这些问题，设计了一种新的服务质量功能调度和分配框架，称为 Sequoia。 Sequoia 允许开发人员或管理员根据易于配置的灵活策略轻松定义无服务器功能和应用程序的部署、限制、优先级或更改方式。受控和现实工作负载的结果表明，Sequoia 可以无缝适应策略，消除链中掉线，将排队时间减少多达 6.4 倍，实施严格的链级公平性，并将运行时性能提高多达 25 倍。

##### 7.Particle: ephemeral endpoints for serverless networking

**摘要：**

突发并行无服务器应用程序调用数千个短暂的分布式函数来完成复杂的工作，例如数据分析、视频编码或编译。虽然这些任务可以在几秒钟内执行，但启动和配置它们所依赖的虚拟网络是一个主要瓶颈，可能会占用高达 84% 的总启动时间。在本文中，我们在三个流行的覆盖网络 Docker Swarm、Weave 和 Linux Overlay 中描述了这种网络冷启动问题的严重程度。我们专注于端到端的启动时间，包括启动一组容器的时间以及互连它们的时间。我们的主要观察结果是，现有的无服务器网络覆盖方法在短暂的无服务器环境中扩展性很差。根据我们的发现，我们开发了 Particle，这是一种专为多节点无服务器覆盖网络量身定制的网络堆栈，可在不牺牲多租户、通用性或吞吐量的情况下优化网络创建。当集成到无服务器突发并行视频处理管道中时，Particle 将应用程序运行时间提高了 2.4--3 倍于现有覆盖。

#### 2019

> 2019.11.20-11.23

##### 1.Narrowing the Gap Between Serverless and its State with Storage Functions

**摘要：**

无服务器计算因其细粒度供应、大规模多租户和按需扩展而受到关注。但是，它也迫使应用程序将远程存储中的状态外部化，从而增加了大量开销。为了解决这个“数据传输问题”，我们构建了 Shredder，这是一个低延迟的多租户云存储，允许直接在存储节点内执行小型计算单元。存储租户为 Shredder 提供 JavaScript 函数（或 WebAssembly 程序），这些函数可以直接与数据交互，而无需通过网络移动它们。

Shredder 的主要挑战是安全隔离数千个租户存储功能，同时最大限度地降低数据交互成本。 Shredder 使用一种独特的方法，其数据存储和网络路径在本机代码中实现以确保性能，而隔离的租户功能使用 V8 特定的中间表示与数据交互，避免了昂贵的跨保护域调用和数据复制。因此，Shredder 每秒可以执行 400 万个远程调用的租户函数，分布在数千个租户中，中位数和 99% 的响应延迟分别小于 50 微秒和 500 微秒。我们的评估表明，与传统远程存储相比，Shredder 在获取它们之间只有一到三个数据依赖关系的项目时实现了 14% 到 78% 的加速。我们还展示了 Shredder 在加速数据密集型应用程序方面的有效性，包括显示数量级增益的社交图上的 k-hop 查询。

##### 2.Cirrus: a Serverless Framework for End-to-end ML Workflows

**摘要：**

机器学习 (ML) 工作流程极其复杂。典型的工作流由用户交互的不同阶段组成，例如预处理、训练和调整，这些阶段由用户重复执行，但具有异构的计算要求。这种复杂性使 ML 用户难以正确配置和管理资源，并且在实践中构成了一个重大负担，经常导致过度配置并损害用户生产力。一般来说，无服务器计算是解决资源管理问题的一个引人注目的模型，但由于对本地资源的重大限制，将其用于现有的 ML 框架存在许多挑战。

这项工作提出了 Cirrus——一个 ML 框架，它通过有效利用无服务器基础架构来自动化 ML 工作流的数据中心资源的端到端管理。 Cirrus 结合了无服务器接口的简单性和无服务器基础设施（AWS Lambdas 和 S3）的可扩展性，以最大限度地减少用户工作量。我们展示了一种专门用于无服务器计算和迭代 ML 训练的设计，以便在无服务器基础架构上进行稳健高效的 ML 训练。我们的评估表明，Cirrus 优于单一维度的专门框架：Cirrus 比通用无服务器系统 [36] 快 100 倍，比传统基础设施的专门 ML 框架快 3.75 倍 。

##### 3.Practical Cloud Workloads for Serverless FaaS

**摘要：**

无服务器计算在功能即服务 (FaaS) 执行模型中越来越受欢迎。无服务器计算不会产生与配置云实例相关的开销，并且具有高可用性和可扩展性，使开发人员可以专注于使用其他开发良好的云服务来实现核心应用程序逻辑。通过抽象复杂的资源管理任务，无服务器计算为云服务的采用开辟了新的机会，甚至对非云专家 [2]。随着流行，许多研究成果已经使用 FaaS 执行模型发表。它们包括调查无服务器计算机会 [1]、提出新的无服务器应用程序、功能运行时优化和公共服务比较。在没有通用测试基准套件的情况下，之前工作中的作者使用相当简单的 FaaS 应用程序评估了提议的系统，例如专门强调特定资源的微基准测试，例如 CPU、磁盘 I/O 和网络。然而，如此简单的工作负载并不代表真实的 FaaS 系统应用程序，并且评估可能无法适当地比较提议的系统。

为了克服无服务器计算和 FaaS 执行模型缺乏综合基准套件的限制，作者创建了 FunctionBench，它提供了各种 FaaS 工作负载，可以在公共云功能执行服务——AWS Lambda、Google Cloud Functions 和 Azure 上执行功能1.自从服务于 FaaS 工作负载以来，我们一直在努力扩展支持的应用程序，并在大数据处理、后端 Web 应用程序和安全性方面添加场景。为了表示大数据应用程序，我们添加了 MapReduce WordCount 工作负载，它计算来自维基百科的给定分区输入数据集中每个单词的出现次数。为了涵盖 Web 后端应用程序，我们添加了 Chameleon。应用程序使用 Python PIP 库中的 Chameleon 模块呈现模板，以创建作为输入参数提供的 N 行和 M 列的 HTML 表。另一个与 Web 相关的应用程序是 JSON 序列化反序列化模块。该应用程序使用从公共对象存储服务下载的 JSON 编码字符串数据集（Awesome JSON Dataset）执行 JSON 反序列化，并再次序列化 JSON 对象。为了表示与安全相关的应用程序，我们添加了执行基于私钥的加密和解密的 Pyaes 基准测试。它是 CTR 模式下 AES 分组密码算法的纯 Python 实现。我们还添加了 gzip 压缩基准来表示现实的磁盘 IO 繁重的应用程序。表1总结了新提出的应用程序的资源使用特性程度（高、中、低）。请参阅[3]阅读综合应用程序列表的描述。

提议的FunctionBench提供了多个类别的多种FaaS应用，我们相信它将使相关领域的新研究工作与实际应用场景得到公平的评价。

### CLUSTER

> 2022年截稿日期 2022.7.8

#### 2021

> 2021.9.7-10

##### 1.Tackling Cold Start of Serverless Applications by Efficient and Adaptive Container Runtime Reusing [8]

**摘要：**

在过去几年中，无服务器计算由于其独特的优势，包括易于管理、自动扩展、内置容错等，已经改变了云和边缘的应用开发和部署模式。尽管如此，无服务器计算也面临着挑战，如**冷启动带来的长延迟**。在本文中，我们对无服务器框架中的冷启动进行了深入的性能分析，并提出了HotC，一个基于容器的运行时管理框架，利用轻量级容器来缓解冷启动，提高无服务器应用程序的网络性能。**HotC 维护一个实时的容器运行时池，分析用户输入或配置文件，并提供可用的运行时供立即重用**。为了精确预测请求并有效地管理热容器，我们设计了一种结合指数平滑模型和马尔科夫链方法的自适应实时容器控制算法。我们的评估结果表明，HotC引入的开销可以忽略不计，并且可以有效地提高云服务器和边缘设备中不同网络流量模式的各种应用的性能。

##### 2.Supporting Elastic Compaction of LSM-tree with a FaaS Cluster [1]

**摘要：**

LSM-tree 在许多键值存储中被广泛用作写优化存储引擎。但是，L**SM-tree 中的周期性压缩操作会消耗大量 I/O 带宽和本地服务器的 CPU 资源，导致系统吞吐量下降**。针对这个问题，本文提出了一种新的基于FaaS（Functions as a Service）集群的compaction方案，称为FaaS Compaction。它**利用 FaaS 集群的弹性计算能力，将压缩推送到 FaaS 集群**。 FaaS集群会进行实际的compaction操作，不会影响本地服务器的处理。因此，即使触发了周期性压缩，我们也可以保持稳定的性能。我们实现了 FaaS 压缩并将 FaaS 压缩与 RocksDB 和最先进的卸载压缩策略进行比较。结果表明了我们提议的效率和弹性。

> 2019年及以前没有serverless相关的文章

### ICDCS

> 截稿日期：2022.1.31

#### 2021

> 2021.7.7-7.10

##### 1.Defuse: A Dependency-Guided Function Scheduler to Mitigate Cold Starts on FaaS Platforms [10]

**摘要:**

函数即服务 (FaaS) 正在成为开发云应用程序的流行范式。使用 FaaS，客户可以将应用程序开发为无服务器函数，将资源管理的负担留给云提供商。但是，FaaS 平台会遭受**函数冷启动**导致的性能下降。当无服务器函数在加载到内存之前被调用时，就会发生冷启动。这个问题是不可避免的，因为数据中心的内存通常太有限，无法同时容纳所有无服务器函数。冷函数调用的延迟会极大地降低 FaaS 平台的性能。

目前，FaaS 平台采用各种调度方法来减少冷启动的发生。但是，他们没有考虑无服务器函数之间普遍存在的依赖关系。观察到使用依赖项来缓解冷启动的潜力，我们提出了 **Defuse，这是一个在 FaaS 平台上的依赖项引导的函数调度程序。具体来说，Defuse 识别了无服务器函数之间的两种依赖类型，即强依赖和弱依赖。它使用频繁模式挖掘和正点互信息分别从函数调用历史中挖掘这种依赖关系**。这样，Defuse 就构造了一个函数依赖图。可以安排图表上的连接组件（即依赖函数）以减少冷启动的发生。我们通过将 Defuse 应用于工业无服务器数据集来评估其有效性。实验结果表明，与最先进的方法相比，Defuse 可以减少 22% 的内存使用量，同时函数冷启动率降低 35%。

##### 2.Gillis: Serving Large Neural Networks in Serverless Functions with Automatic Model Partitioning [1]

**摘要：**

深度神经网络的使用增加刺激了对基于云的模型服务平台的需求不断增长。无服务器计算提供了一种简化的解决方案：用户将模型部署为无服务器函数，并让平台处理供应和扩展。然而，**无服务器函数限制了 CPU 和内存中的资源，使其效率低下或无法为大型神经网络提供服务**——这已变得越来越流行。在本文中，我们介绍了 **Gillis，这是一个基于无服务器的模型服务系统，它自动将大型模型划分为多个无服务器函数，以加快推理速度并减少每个函数的内存占用**。 Gillis 采用了两种新颖的模型划分算法，分别实现了延迟最优服务和成本最优服务，并符合 SLO。我们已经在三个无服务器平台（AWS Lambda、Google Cloud Functions 和 KNIX）上实现了 Gillis，并将 MXNet 作为服务后端。针对流行模型的实验评估表明，Gillis 支持服务非常大的神经网络，大大降低了推理延迟，并以较低的服务成本满足了各种 SLO。

##### 3.Poster: Function Delivery Network: Extending Serverless to Heterogeneous Computing

**摘要：**

当今的一些云计算应用分布在异构连接的计算资源上，并且在结构和资源需求上高度动态化。然而，无服务器计算和函数即服务**（FaaS）平台仅限于同构集群和同构函数**。我们**介绍了FaaS对异构计算的扩展，并通过分布式异构目标平台的网络支持异构函数，称为功能交付网络（FDN）。 目标平台是同构计算系统的集群和其上的FaaS平台的组合。**FDN提供函数交付即服务（FDaaS），将函数调用交付给合适的目标平台。我们展示了FDN在实现两个目标方面提供的机会，如多个目标平台之间的协作执行和不同目标平台的特性。我们通过对五个分布式目标平台的评估，展示了FDN在满足两个目标方面提供的机会，如多个目标平台之间的协作执行和不同的目标平台特性：服务水平目标（SLO）要求和调度功能调用时的能源效率。

##### 4.A Multi-Tenant Framework for Cloud Container Services

**摘要：**

云原生时代，容器技术发展迅速。 Kubernetes 作为生产级容器编排平台，已被证明在管理本地数据中心的容器化应用程序方面取得了成功。然而，**Kubernetes 在设计上缺乏足够的多租户支持，这意味着在云环境中，需要专用集群来服务多个用户，即租户**。这种限制显着削弱了云计算的优势，并使得使用 Kubernetes 构建多租户软件即服务 (SaaS) 产品变得困难。在本文中，我们**提出了 Virtual-Cluster，这是一种新的多租户框架，它通过足够的多租户支持扩展了 Kubernetes。基本上，虚拟集群提供控制平面和数据平面隔离，同时在租户之间共享底层计算资源**。新框架通过避免修改 Kubernetes 核心组件来保持 API 兼容性。因此，它可以很容易地与现有的 Kubernetes 用例集成。我们的实验结果表明，VirtualCluster 引入的开销在延迟和吞吐量方面是适中的。

#### 2020

> 2020.11.29-12.1

##### 1.λ-NIC: Interactive Serverless Compute on Programmable SmartNICs [2]

**摘要：**

人们对无服务器计算越来越感兴趣，这是一种云计算模式，可以自动分配和管理基础设施资源，同时只对客户使用的资源收费。像流处理这样的工作负载得益于这些无服务器框架的高弹性和细粒度的定价。然而，到目前为止，**服务器CPU有限的并发性和高延迟使许多交互式工作负载（如网络服务器和数据库客户端）无法利用无服务器计算实现高性能**。在本文中，**我们认为服务器CPU不适合运行无服务器工作负载（即 λ-NIC是一个开源框架，它直接在SmartNIC上运行交互式工作负载；更确切地说，是一个基于ASIC的NIC，由密集的网络处理单元（NPU）内核组成。λ-NIC利用SmartNIC靠近网络的特点和大量的NPU内核，在一个NIC上同时运行成千上万的lambdas，并有严格的尾部延迟保证**。为了简化lambdas的开发和部署，λ-NIC公开了一个基于事件的编程抽象，即Match+Lambda，以及一个机器模型，允许开发者在SmartNIC上轻松地编排和执行lambdas。我们的评估显示，λ- NIC在工作负载的响应延迟和吞吐量方面分别实现了高达880倍和736倍的改进，同时大大减少了主机CPU和内存的使用。

##### 2.Serverless Straggler Mitigation using Error-Correcting Codes

**摘要：**

廉价的云服务，如**无服务器计算，往往容易受到滞留节点的影响，从而增加分布式计算的端到端延迟**。我们在无服务器系统中**提出并实施了简单而有原则的矩阵乘法的滞留缓解方法**，并在机器学习和高性能计算的几个常见应用中进行了评估。所提出的方案受到纠错码的启发，采用无服务器工作者对存储在云中的数据进行并行编码和解码。这创造了一个完全分布式的计算框架，不需要使用主节点来进行编码或解码，这消除了主节点的计算、通信和存储瓶颈。在理论方面，我们建立了我们提出的方案在解码时间上是渐进最优的，并提供了它能容忍的高概率的散兵人数的下限。通过广泛的实验，我们表明我们的方案比现有的方案，如投机执行和其他编码理论方法，至少要好25%。

##### 3.Characterizing Bottlenecks in Scheduling Microservices on Serverless Platforms

**摘要：**

数据中心正目睹着越来越多的趋势，即采用基于微服务的架构进行应用设计，它由不同的微服务组合而成。通常情况下，这些应用的寿命很短，并且在管理上有严格的服务水平目标（SLO）要求。传统的基于虚拟机（VM）的配置对于这类应用来说，不仅在配置资源时存在较长的延迟（因为虚拟机往往需要几分钟才能启动），而且还将服务器管理和配置的额外开销放在用户身上。这导致了无服务器功能的采用，在这种情况下，应用程序被组成为函数并托管在容器中。然而，**无服务器平台中采用的最先进的调度器倾向于将基于微服务的应用与传统的单体黑盒应用类似**。为了检测所有的低效率，我们在这项工作中描述了这些基于微服务的应用程序的端到端生命周期。我们的研**究结果表明，由于在工作负载波动期间的反应性容器供应，这些应用遭受了微服务的不良调度，从而导致违反SLO或巨大的容器过度供应，反过来导致资源利用率低**。我们还发现，在应用程序执行的每个阶段都有大量的松弛，这有可能被用来提高应用程序的整体性能。

> 2019年及以前没有serverless方向文章

### EuroSys

#### 2021

> 2021.426-4.28

##### 1.OFC: an opportunistic caching system for FaaS platforms

**摘要：**

基于 "函数即服务"（FaaS）范式的云应用已经变得非常流行。然而，**由于它们的无状态性质，它们必须经常与外部数据存储互动，这限制了它们的性能**。为了缓解这个问题，我们引入了**OFC，一个透明的、垂直和水平弹性的内存缓存系统**，用于FaaS平台，分布在工作节点上。OFC通过利用两种常见的资源浪费来源，以成本效益的方式提供这些好处。(i) 大多数云租户过度配置为其功能预留的内存资源，因为它们的足迹与输入无关；(ii) FaaS供应商将函数沙箱保持几分钟，以避免冷启动。使用针对典型函数输入数据类别（如多媒体格式）调整的机器学习模型，OFC估计每个函数调用所需的实际内存资源，并囤积剩余的容量以供给缓存。我们基于对OpenWhisk FaaS平台、Swift持久性对象存储和RAM-Cloud内存存储的改进来建立我们的OFC原型。通过使用一组不同的工作负载，我们表明OFC可以将单级和流水线函数的执行时间分别提高82%和60%。

#### 2020

##### 1.A fault-tolerance shim for serverless computing

**摘要：**

近年来，无服务器计算越来越受欢迎，越来越多的应用程序被构建在函数即服务（FaaS）平台上。默认情况下，**FaaS平台支持基于重试的容错，但这对于修改共享状态的程序来说是不够的，因为它们会在不知不觉中坚持部分更新集，以防出现故障**。为了应对这一挑战，我们希望FaaS应用程序所做的更新具有原子可见性。

在本文中，我们介绍了无服务器应用的原子容错垫片 aft。aft介于商品FaaS平台和存储引擎之间，通过执行读原子隔离保证来确保更新的原子可见性。aft支持新协议，以保证无服务器环境中的读原子隔离。我们证明，相对于现有的存储引擎，aft引入了最小的开销，并能平稳地扩展到每秒数千个请求，同时防止了大量的一致性异常情况。

##### 2.SEUSS: skip redundant paths to make serverless fast

**摘要：**

本文介绍了一种在FaaS环境中实现无服务器函数的快速部署和高密度缓存的系统级方法**。为了减少启动时间，函数从单内核快照中部署，绕过了昂贵的初始化步骤。为了减少快照的内存占用，我们在运行一个函数所需的整个软件栈中应用了页级共享。**我们通过在FaaS平台架构的计算节点上替换Linux来证明我们技术的效果。使用我们的原型操作系统，一个函数的部署时间从100多毫秒下降到10毫秒以下。在完全由新函数组成的工作负载上，平台的吞吐量提高了51倍。我们**能够在内存中缓存超过50,000个函数实例，而使用标准的操作系统技术只能缓存3,000个**。综合来看，这些改进使FaaS平台具有处理大规模突发请求的新能力。

> 2019年以及之前不含serverless方向的文章

### ICPP

#### 2021

> 2021.8.9-8.12

##### 1.AMPS-Inf: Automatic Model Partitioning for Serverless Inference with Cost Efficiency

无服务器计算突出的按使用付费的特性，推动了其作为各种工作负载的替代计算范式的不断渗透。然而，在**将机器学习工作负载转移到无服务器环境中时，出现了一些挑战，并且仍然没有解决**。具体来说，无服务器平台对部署规模的限制与神经网络模型的复杂性相结合，使得在单个无服务器功能中部署大型模型变得困难。在本文中，我们**旨在充分发挥无服务器计算范式在机器学习工作负载中的优势，在满足响应时间服务水平目标（SLO）的同时，减轻管理和整体成本**。我们设计并实现了AMPS-Inf，一个为无服务器计算中的模型推理而定制的自主框架。在成本效益和及时响应的驱动下，我们提出的AMPS-Inf为推理工作负载自动生成最佳执行和资源配置计划。AMPS-Inf的核心依赖于混合整数二次编程问题的制定和解决，用于模型分区和资源配置，目标是在不违反响应时间SLO的情况下实现成本最小化。我们在AWS Lambda平台上部署了AMPS-Inf，用Keras中最先进的预训练模型进行评估，包括ResNet50、Inception-V3和Xception，并与Amazon SageMaker和三个基线进行比较。实验结果表明，AMPS-Inf在不降低响应时间性能的情况下实现了高达98%的成本节约。

### IPDPS

#### 2021

> 2021.5.17-5.21

##### 1.Astra: Autonomous Serverless Analytics with Cost-Efficiency and QoS-Awareness.

由于能够通过一键上传和轻量级执行来简化代码部署，无服务器计算已经成为一种很有前途的范式，并且越来越受欢迎。然而，在使**数据密集型分析应用适应无服务器环境时，仍然存在公开的挑战**，无服务器分析的用户在协调不同阶段的计算以及在大的配置空间中配置资源方面遇到了困难。本文介绍了我们对Astra的设计和实现，它以自主方式配置和协调无服务器分析工作，同时考虑到灵活指定的用户需求。Astra依赖于性能和成本的建模，该模型描述了多维因素（例如，函数内存大小、每个阶段的并行程度）之间错综复杂的相互作用。我们根据用户的具体要求制定了一个优化问题，以提高性能或降低成本，并**开发了一套基于图论的算法，以获得最佳作业执行**。我们在AWS Lambda平台上部署了Astra，并在三个不同规模的代表性基准上进行了真实世界的实验。结果表明，与三种基线配置算法相比，Astra可以实现无服务器分析的最佳执行决策，在给定的预算约束下提高21%到60%的性能，并在不违反性能要求的情况下降低20%到80%的成本。

#### 2020

> 2020.5.18-5.22

##### 1.Amoeba: QoS-Awareness and Reduced Resource Usage of Microservices with Serverless Computing

虽然有严格的服务质量限制的微服务被部署在云中，但由于昼夜负载模式，除了高峰时段，承载微服务的长期租用的基础设施的利用率很低。在高负荷时将微服务部署在长期基础设施中，在低负荷时将微服务部署在无服务器计算平台中，这对云计算供应商来说具有资源效率，对服务维护者来说具有成本效率。然而，先前的工作未能利用这一机会，因为**无服务器平台上的微服务之间的争夺严重影响了它们的响应延迟**。我们的调查显示，微服务的负载、无服务器平台上的共享资源争夺及其对争夺的敏感度共同影响了平台上微服务的响应延迟。为此，我们**提出了Amoeba，一个能动态切换微服务部署的运行时系统。Amoeba由一个竞争意识的部署控制器、一个混合执行引擎和一个多资源竞争监视器组成。部署控制器根据微服务的负载和无服务器平台上的争用情况预测微服务的尾部延迟，并确定微服务的适当部署**。混合执行引擎可以实现两种部署模式的快速切换。争用监视器定期对多种类型的共享资源的争用情况进行量化。实验结果表明，与传统的基于IaaS的纯部署方式相比，Amoeba能够显著减少高达72.9%的CPU使用量和高达84.9%的内存使用量，同时确保达到所需的延迟目标。

> 2019年及以前没有serverless领域的文章

### HPDC

#### 2021

> 2021.6.21-6.25

##### 1.A Serverless Framework for Distributed Bulk Metadata Extraction

我们介绍**Xtract，一个用于从大型分布式研究数据库中批量提取元数据的自动化和可扩展的系统**。Xtract协调元数据提取器在文件组中的应用，决定对每个文件应用哪些提取器，以及对每个提取器和文件在哪里执行。建立在funcX联合FaaS平台上的混合计算模型，使Xtract能够通过将每个提取任务分配到最合适的位置来平衡提取时间和数据传输成本之间的权衡。在一系列云和超级计算机上的实验表明，Xtract可以通过协调成千上万个节点上的基于容器的提取器的并发执行，有效地处理数百万个文件库。我们通过将Xtract应用于一个大型的、半整理的科学数据库和一个未整理的科学Google Drive数据库来强调Xtract的灵活性。我们表明，通过在分散的存储和计算节点上远程协调元数据提取，Xtract可以在50%的时间内处理大型存储库，而这只是将相同的数据传输到同一计算设施内的机器上。我们还表明，当传输数据是必要的（例如，没有本地计算），Xtract可以扩展到处理文件的速度，甚至通过一个多GB/s的网络。

##### 2.LaSS: Running Latency Sensitive Serverless Computations at the Edge

无服务器计算已经成为在云中运行短暂计算的新范式。由于其处理物联网工作负载的能力，人们对在边缘运行无服务器功能产生了相当大的兴趣。然而，**边缘的约束性和工作负载的延迟敏感性给无服务器平台带来了许多挑战**。在本文中，我们介绍了**LaSS，一个使用模型驱动的方法在边缘资源上运行延迟敏感的无服务器计算的平台**。LaSS使用基于排队的原则性方法来确定对每个托管功能的适当分配，并根据工作负载的动态变化自动扩展分配的资源。LaSS使用公平分享的分配方法，保证在过载的情况下为每个功能分配最小的资源。此外，它利用基于容器放空和终止的资源回收方法，将资源从超额配置的功能中重新分配给不足配置的功能。我们在OpenWhisk无服务器边缘集群上实现了我们方法的原型，并进行了详细的实验评估。我们的结果表明，LaSS可以准确地预测无服务器功能在高动态工作负载下所需的资源，并在数百毫秒内重新配置容器容量，同时保持公平份额的分配保证。

> [关于serverless的研讨会](https://dblp.uni-trier.de/db/conf/hpdc/hips2021.html)

# CCFC

## 会议

### HPCC

#### 2021

##### 1.Descriptive and Predictive Analysis of Aggregating Functions in Serverless Clouds: the Case of Video Streaming

无服务器云将多个用户的多个任务（如微服务）分配到一个共享的计算资源池中。这使得无服务器云提供商能够通过透明地聚合某种背景下的类似任务（如视频处理）来减少其资源使用量，这些任务共享其全部或部分计算。为此，了解通过聚合任务实现的时间节省量是至关重要的。缺乏这样的知识会导致不知情的合并和调度决策，反过来又会导致合并后的任务或其他后续任务违反最后期限。因此，**在本文中，我们以视频处理为例，研究了估算合并任务带来的执行时间节省问题**。为了了解不同形式的合并所节省的执行时间，我们首先建立了一组基准视频，并检查了各种各样的视频处理任务--有无合并。我们观察到，尽管合并可以节省44%的执行时间，但可能的合并案例的数量是难以解决的。因此，在第二部分，我们利用基准测试结果，开发了一种基于梯度提升决策树（GBDT）的方法来估计任何给定任务合并情况下的时间节省。实验结果表明，根据均方根误差（RMSE）衡量，该方法可以估计出节省的时间，误差率为0.04。

### HiPC

#### 2021

##### 1.FaaSter: Accelerated Functions-as-a-Service with Heterogeneous GPUs

在这项工作中，我们提出了FaaSter，一个加速函数即服务（AFaaS）产品，它将函数即服务模式与GPU加速资源统一起来。**FaaSter提供了一个加速函数库作为服务，而这又是在异构的GPU上提供的**。为了提供对这些加速函数的无缝访问，并确保每个函数具有最佳的响应时间，我们利用GPU内核切片，在多个异构GPU上分割和执行一个加速函数实例。核心挑战是要能够快速决定将每个函数分割成多少片，然后将这些片映射到合适的GPU上。为此，我们提出了一种调度启发式方法，与非切片的全GPU调度方法相比，能够显著减少函数的平均周转时间。我们的评估结果显示，FaaSter调度器在平均周转时间方面实现了62%的平均改善，最高可达80%，并且总是与非可分片的GPU调度表现相当或更好。

### CCGRID

> 截止日期2021.12.19

#### 2021

> 2021.5.10-2021.5.13

##### 1.Data-driven scheduling in serverless computing to reduce response time [10]

在函数即服务 (FaaS) 中，一种无服务器计算变体，客户部署函数而不是完整的虚拟机或 Linux 容器。维护这些函数的运行时环境的是云提供商。所有主要云提供商（例如 Amazon Lambda、Google Cloud Functions、Azure Functions）都提供 FaaS 产品；以及独立的开源软件（例如 Apache OpenWhisk）及其商业变体（例如 Adobe I/O Runtime 或 IBM Cloud Functions）。**我们采用 FaaS 集群中单个节点的自下而上的视角。我们假设已经安装了分配给该节点的一组函数的所有执行环境。我们的目标是安排由负载均衡器传递的函数的单独调用，以最小化与响应时间相关的性能指标**。部署的函数通常会重复执行以响应最终用户的多次调用。因此，我们的调度决策基于本地收集的信息：记录的调用频率和执行时间。我们提出了一些启发式方法，并且我们还采用了一些具有理论基础的启发式方法，例如 SEPT 或 SERPT。我们的**模拟使用最近发布的 Azure Functions Trace**。我们表明，与基线 FIFO 或循环相比，我们的数据驱动调度决策显着提高了性能。

##### 2.Deadline-aware Dynamic Resource Management in Serverless Computing Environments [10]

无服务器计算通过大规模组合松散耦合的微服务来实现快速的应用程序开发和部署。这种新兴的范式极大地减轻了云环境用户的负担，无需提供和管理底层云资源。随着责任的转移，云**提供商面临的挑战是在不影响可靠性的情况下向用户提供可接受的性能**，同时对应用程序要求的了解最少。次优资源分配，特别是 CPU 资源，可能会导致违反应用程序的性能要求。此外，细粒度的无服务器计费模型仅根据函数执行时间对资源使用收费。同时，提供者必须将底层基础设施保持在永远在线模式，以方便异步函数调用。因此，在不影响应用程序需求的情况下实现云资源的最佳利用对提供商来说非常重要。**当前的大部分工作只关注最小化由基础设施设置延迟引起的函数执行时间，并降低最终用户的资源成本**。然而，在本文中，我们**同时关注提供者和用户的观点**，并为部署在无服务器计算环境中的应用程序提出了函数放置策略和动态资源管理策略。这些策略在满足用户的应用需求（即期限）的同时，最大限度地降低了服务提供者的资源消耗成本。所提出的解决方案对截止日期很敏感，并有效地提高了提供商的资源利用率，同时动态管理资源以改善功能响应时间。我们**使用 ContainerCloudSim 工具包通过模拟来实施和评估我们的方法**。与基线调度技术相比，所提出的功能放置策略可以将资源消耗减少多达三倍。使用固定资源分配策略和比例 CPU 份额策略评估动态资源分配策略时，在满足所需的函数期限方面表现出高达 25% 的改进。

##### 3.Benchmarking Serverless Workloads on Kubernetes [10]

作为云领域的颠覆性范式，无服务器计算因其降低运营成本和外包基础设施管理的独特价值主张而备受关注。然而，**企业函数即服务 (FaaS) 平台可能会带来重大风险，例如供应商锁定、多租户导致缺乏安全控制、复杂的定价模型以及法律和法规遵从性**——尤其是在移动计算场景中。这项工作提出了一种生产级容错无服务器架构，该架构基于使用开源框架的高可用性 Kubernetes 拓扑，部署在 OpenStack 实例上，并以**现实中的缩小的 Azure 工作负载跟踪数据集为基准**。通过测量成功率、吞吐量、延迟和自动可扩展性，我们成功地评估了三种不同代表性工作负载在逻辑模型下的弹性和持续性能。我们的测试执行表明，有 95% 的置信度，70 到 90 个并发用户可以访问系统，同时体验可接受的性能。超出确定的断点（即每秒 91 个事务），Kubernetes 集群必须扩展或扩展以满足 QoS 和可用性要求。

##### 4.Algorithms for scheduling scientific workflows on serverless architecture

无服务器计算是一种新颖的云计算范式，云提供商管理底层基础设施，而用户只需要上传应用程序的代码。功能即服务 (FaaS) 是一种无服务器计算模型，其中短期方法在云中执行。 FaaS 有前途的用例之一是运行科学工作流应用程序，它代表了由相关任务组成的科学过程。由于 FaaS 的独特功能，包括快速资源供应、间接基础设施管理和细粒度计费模型，因此需要创建专用的调度方法，以有效地将新型基础设施用作工作流应用程序的环境。在本文中，我们**提出了两种新颖的调度算法 SMOHEFT 和 SML，它们旨在创建一个时间表，用于在无服务器基础设施上执行有关时间和成本限制的科学工作流**。我们通过执行实验来评估提出的算法，我们计划执行三个应用程序：Ellipsoids、Vina 和 Montage。 SDBWS 和 SDBCS 算法被用作基线。 SML 在执行 Ellipsoids 工作流时取得了最好的结果，成功率在 80% 以上，而其他算法在 60% 以下。在 Vina 的情况下，除 SDBWS 之外的所有算法的成功率都在 87.5% 以上，而在 Montage 的情况下，所有算法的成功率相似，都在 87.5% 以上。所提出的算法的成功率与其他研究解决方案提供的相当或更好。

##### 5.High Performance Serverless Architecture for Deep Learning Workflows

无服务器架构是一种快速发展的范例，用于部署执行临时计算和服务突发工作负载的深度学习应用程序。无服务器架构保证了推理深度学习模型的自动扩展和成本效率，同时最小化操作逻辑。但是，无服务器计算是无状态的，对本地资源有限制。因此，部署包含大型模型、框架和库的复杂深度学习应用程序是一项挑战。在这项工作中，我们**讨论了将深度视觉算法和基于模型的应用程序迁移到无服务器计算平台的方法和架构**。我们使用 AWS 基础设施（AWS Lambda、预置并发、VPC 端点、S3 和 EFS）测试了我们的方法，以缓解部署包含大型深度学习模型和框架的 API 组合的挑战。我们针对用于文档处理的真实企业应用程序评估我们的架构的性能和成本。

##### 6.A Reinforcement Learning Approach to Reduce Serverless Function Cold Start Frequency [10]

无服务器计算是一种事件驱动的云计算架构，用于按需处理请求，使用轻量级函数容器和微服务模型。物联网 (IoT) 服务、边缘计算和流处理等各种应用程序已被引入到无服务器范式中。这些应用程序的特点是它们对响应时间的要求很严格，因此期望应用程序能够提供快速且容错的反馈。无服务器或函数即服务 (FaaS) 范式面临功能“冷启动”挑战，其中无服务器平台需要时间来设置依赖项、准备运行时环境和代码以供执行，然后再为传入的工作负载提供服务。**目前大多数工作通过（1）减少函数容器的启动或准备时间，或（2）减少平台上函数冷启动的频率来解决冷启动问题**。最近的工业研究发现，诸如运行时环境、CPU 和内存设置、调用并发和网络要求等因素会影响函数的冷启动。**因此，我们提出了强化学习（Q-Learning）代理设置，通过提前准备函数实例来分析识别的函数 CPU 利用率等因素，确定函数调用模式并降低函数冷启动频率**。所提出的 Q-Learning 代理通过分别使用每个实例的 CPU 利用率、可用的函数实例和响应的成功或失败率来离散化环境状态、动作和奖励，与 Kubeless 无服务器平台交互。使用 Apache JMeter 非 GUI 工具包复制工作负载，并根据 Kubeless 的基线默认自动扩展功能评估我们的代理。代理展示了学习调用模式的能力，通过在受控环境设置下在学习期间准备最佳数量的函数实例来做出明智的决策。

##### 7.AI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments

近年来，无服务器计算已成为一种引人注目的云计算模型新范式。它承诺以大规模和低成本为用户提供服务，同时消除对基础设施管理的需求。在云提供商方面，需要灵活的资源管理来满足波动的需求。它可以通过自动配置和取消配置资源来启用。商业和开源无服务器计算平台之间的一种常见方法是基于工作负载的自动缩放，其中指定的算法根据传入请求的数量来缩放实例。在最近发展的无服务器框架 Knative 中，提出了一种基于请求的策略，其中算法通过配置的最大请求数来扩展资源，每个实例可以并行处理，即所谓的并发。正如我们在基线实验中所展示的，这种预定义的并发级别会强烈影响无服务器应用程序的性能。但是，由于各种因素，例如，识别产生最高服务质量的并发配置是一项具有挑战性的任务。不同的工作负载和复杂的基础设施特征，影响吞吐量和延迟。尽管已经对用于优化虚拟机配置的自动扩展的智能技术进行了大量研究，但该主题尚未在无服务器计算领域进行讨论。出于这个原因，**我们研究了强化学习方法在无服务器框架中对基于请求的自动扩展的适用性**。我们的结果表明，在有限的迭代次数内，我们提出的模型学习了每个工作负载的有效扩展策略，与默认的自动扩展配置相比，提高了性能。

##### 8.Scheduling Containers Rather Than Functions for Function-as-a-Service

函数即服务 (FaaS) 是一项引人注目的技术，它允许用户以事件驱动的方式运行函数，而无需担心服务器管理。基于容器的虚拟化使函数能够在轻量级和隔离的运行时环境中运行，但伴随着容器初始化（冷启动）的频繁函数执行会使平台繁忙且无响应。出于性能考虑，鼓励暖启动，即在已经初始化的容器上执行函数，因此 FaaS 平台努力将函数调度到暖容器。根据我们操作本地 FaaS 平台的经验，我们**发现现有的调度程序对多租户和高并发工作负载表现出较差的性能和不稳定的行为**。本文提出了一种**新的 FaaS 调度算法，名为 FPCSch**，它调度 Function-Pulling-Containers，而不是将函数调度到容器。由于 **FPCSch 让容器不断拉取相同类型的函数，冷启动显着减少**。我们的评估表明，配备 FPCSch 的 Apache OpenWhisk 具有许多适合 FaaS 平台的特性； (1) 相对于越来越多的函数类型混合的多租户工作负载，吞吐量相当稳定，(2) 吞吐量增加以增加并发性，(3) 均匀负载平衡资源密集型工作负载，以及 (4) 几乎成比例的性能向外扩展。

##### 9.Virtual Device Model extending NGSI-LD for FaaS at the Edge [1]

智能环境是由越来越多的异质资源和设备组成的，用于收集和处理大量的环境数据。这些活动可以在智能区域的边缘，通过分布式和异构的基础设施进行，以便接近终端用户并优化响应时间。然而，很难定义一个能够支持不同系统之间以及系统和用户之间数据交换的数据模型。本文介绍了智能环境的主要特征，并引入了虚拟设备的概念，即以特定的高级功能为特征的抽象化组件。然后，本文提出了一个数据模型，用于表示和优化智能环境中的虚拟设备的采用。为了更好地解释数据模型的特点和好处，我们参考了一个视频监控用例，其中一个智能摄像机能够提供实心角检测服务。

##### 10.QoS aware FaaS platform [10]

函数即服务（FaaS）是无服务器计算的一种形式，是最近的云计算服务产品之一，它将资源的管理和配置以及应用程序的部署抽象化和自动化。它提供了强大的抽象，将应用程序组成无状态函数，并通过事件触发其执行。该平台为应用程序提供自主扩展，并提供 "即用即付 "的秒级计费模式。然而，当**代FaaS平台在说明资源需求方面提供的功能有限**。它们往往**缺乏表达应用特性和与服务质量（QoS）相关的资源要求的规范**。这样的规范可以有效地指导资源提供者的资源配置和功能部署，从而实现高效的资源利用和成本节约。这项研究探索激发了对FaaS的QoS规范框架的需求，并提出了实现初始QoS感知FaaS平台的想法。基于现实世界工作负载跟踪的实验结果表明，QoS可以为FaaS平台带来成本节约和有效的资源利用。

#### 2020

> 2020.5.11-2020.5-14

##### 1.Performance Optimization for Edge-Cloud Serverless Platforms via Dynamic Task Placement

我们**提出了一个使用动态任务放置的无服务器边缘云平台的性能优化框架**。我们专注于智能边缘设备的应用，例如智能相机或扬声器，它们需要在实时或接近实时的情况下对输入数据执行处理任务。我们的框架允许用户为每个应用任务指定成本和延迟要求，对于每个输入，它决定是在边缘设备上还是在云中执行任务。此外，对于云端执行，该框架确定了满足性能目标所需的容器资源配置。我们利用从AWS Lambda和AWS Greengrass中的无服务器应用中收集的测量数据对我们的框架进行了模拟评估。此外，我们还实现了我们的框架原型，在这些相同的平台上运行。在对我们的原型进行的实验中，我们的模型可以预测平均端到端延迟，误差小于6%，而且与纯边缘执行相比，我们获得了几乎三个数量级的端到端延迟减少。

##### 2.Cost-Effective Malware Detection as a Service Over Serverless Cloud Using Deep Reinforcement Learning

目前云计算的总体趋势，特别是无服务器计算，影响到组织活动的多个方面。各种规模的组织都在将其部分业务从内部过渡到外部，以降低成本并更有效地扩展其业务。网络安全领域也不例外，许多组织都在利用分布式和可扩展的云环境。由于无服务器计算的收费模式是 "随用随付"（即按行动付费），减少所需的计算次数就会转化为巨大的成本节约。这种理解也与恶意软件检测领域有关，在该领域，各组织经常部署多种类型的检测器以提高检测精度。在这项研究中，我们**利用深度强化学习来减少云计算中的计算成本**，只选择性地查询可用的检测器的子集。我们证明，我们的方法不仅对企业内部和基于云的计算架构有效，而且将其应用于无服务器计算可以降低成本一个数量级，同时保持接近最优的性能。

#### 2019

##### 1.Beyond Load Balancing: Package-Aware Scheduling for Serverless Platforms [10]

在函数即服务（FaaS）平台中快速部署和执行云函数是至关重要的，例如，对于微服务架构。然而，**需要大型包或库的函数是臃肿的，而且启动缓慢**。**一种优化方法是在工作节点上缓存软件包，而不是将其与函数捆绑在一起。然而，现有的FaaS调度器是虚有其表的负载均衡器，不知道为响应先前的函数执行而缓存的包，因此不能适当地获得包缓存的好处**。我们**研究了包感知调度的情况**，并提出了PASch，这是一种新颖的调度算法，在调度过程中寻求包的亲和力，以便工作节点可以重新使用预装包的执行环境。PASch利用一致的散列和2种选择的力量，同时积极避免工人过载。我们在OpenLambda框架的新调度器中实现了PASch，并使用模拟和实际实验对其进行评估。当使用PASch而不是负载最小的平衡器时，任务感知到的平均速度提高了1.29倍，而第80个百分点的延迟则提高了23倍。此外，对于本文研究的工作负载，PASch超过了有界负载的一致散列算法--一种最先进的负载平衡算法--产生了1.3倍的平均速度，在第80百分位数时速度提高了1.5倍。

> 2018年及2018年之前没有该serverless方向的论文

### ICPADS

> 2020年没有该serverless方向的论文

#### 2019

> 2019.12.4-12.6

##### 1.Adaptive Function Launching Acceleration in Serverless Computing Platforms [10]

无服务器计算已成为应用程序和服务部署的一种新的引人注目的范式，它使开发人员能够更多地关注业务逻辑而不是基础设施。 Serverless 计算平台使函数容器规模为零，这导致了一个严重的问题，称为**冷启动**。冷启动严重影响了无服务器计算平台的响应能力，并限制了无服务器计算在更广泛的应用程序中的使用和采用。传统策略以牺牲资源为代价来减少冷启动延迟。**如何同时最小化冷启动延迟和减少策略执行的资源消耗是一个具有挑战性的问题**。在本文中，我们首先**提出了一种自适应预热策略（AWUS）来预测函数调用时间并预热函数，从而减少冷启动延迟。我们使用函数链模型来改进AWUS。我们采用细粒度回归方法来更准确地预测函数链中的非第一函数**。其次，我们**提出了一种自适应容器池扩展策略（ACPSS）来减少函数启动时间。我们动态调整容器池的容量，减少资源浪费**。 AWUS 和 ACPSS 协同工作以减少冷启动延迟和资源浪费。最后，我们实现了一个无服务器计算平台并进行了广泛的实验来评估我们的策略。评估结果证明了我们策略的有效性。

> 2018年及以前没有该serverless方向的论文

## 期刊

### JGC

#### 2021

> 2021.03

##### 1.Deployment Management and Topology Discovery of Microservice Applications in the Multicloud Environment [5]

**摘要：**

云计算推动了现代软件应用程序设计的发展。基于微服务架构的应用程序就是一个例子。同时，多云作为一种基础架构战略被企业广泛接受；然而，挑战依然存在。现代应用程序的自主性和可分布性，以及多云基础设施的复杂性，通常使通用应用程序部署管理变得不切实际。这种现象可能会进一步阻碍应用程序的质量和效率。因此，**多云基础设施环境中的部署资源控制和拓扑发现是云计算研究的一个有趣领域**。本文**提出了一个框架来管理多云环境中的应用程序部署**。该框架使用基于策略的部署控制从多云基础架构中自动选择和提供部署资源，随后使用拓扑发现来可视化和验证实际部署。论文中介绍了所提出的框架设计，并实现了概念验证原型。在经验场景中进行实验。实验结果表明，所提出的框架在控制部署资源和呈现跨云的实际部署方面是有效的。

##### 2.Serverless Workflows for Containerised Applications in the Cloud Continuum [7]

**摘要：**

本文介绍了一个开源平台，以**支持跨云连续体的基于科学数据处理工作流的应用程序的无服务器计算（即同时涉及本地和公共云平台来处理在边缘捕获的数据）**。这是通过为 FaaS 平台提供动态资源来实现的，该平台与归零方法兼容，可最大限度地减少具有不同弹性要求的动态工作负载的资源使用和成本。**该平台结合了在本地云上动态部署的自动扩展 Kubernetes 集群和自动云突发入 AWS Lambda 以实现更高级别的弹性**。智能城市公共卫生用例用于评估该平台，负责从捕获的视频中检测未戴口罩的人。面部被模糊以增强本地云中的匿名性，并且通过深度学习模型在 AWS Lambda 中执行此数据驱动的容器化工作流程的检测。结果表明，跨云连续体的混合工作流可以有效地执行本地数据处理以增强法规遵从性，并执行云爆发以提高弹性水平。

##### 3.Highly Complex Resource Scheduling for Stochastic Demands in Heterogeneous Clouds [6]

**摘要：**

为了处理来自世界各地的用户请求，在线服务需要调度资源以满足地理分布式数据中心对各种资源的相应随机需求。为充分发挥云资源优势，挖掘私有基础设施潜力，**最优调度方案应考虑不同类型数据中心的异构性。它导致了一个高度复杂的非线性规划问题**。为了找到有效的解决方案，我们引入了一个相关且简单的问题，以快速获得接近最优的可行解决方案，然后利用具有特殊步骤的差分进化过程来快速解决它。通过使用模拟和真实数据对算法进行测试，我们发现我们的算法优于现有算法，可以增加 34% 以上的收入。
